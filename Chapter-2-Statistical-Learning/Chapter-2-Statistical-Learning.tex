% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\author{}
\date{}

\begin{document}

\hypertarget{header-n0}{%
\subsection{2.1 What is Statistical Learning}\label{header-n0}}

\(Y = f(X) + \epsilon\)

\(\epsilon\) represents the unreducible errors since it contains
parameters about \(Y\) that cannot be measured

\begin{align}
E(Y-\hat{Y})^2 &= E[f(X)+\epsilon-\hat{f}(X)]^2 \\
&= [f(X) - \hat{f}(X)]^2 + Var(\epsilon)
\end{align}

\([f(X) - \hat{f}(X)]^2\) is reducible, while \(Var(\epsilon)\) is not.
\(\epsilon\) will always provide an upper bound in practice.

\textbf{Inference: }Understanding how the parameters affect \(Y\) and
finding \(f\)

\hypertarget{header-n8}{%
\subsubsection{Parametric Methods}\label{header-n8}}

Two-step model-based approach

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Make an assumption about the functional form of \(f\), ex. that \(f\)
  is linear
\item
  Fit or train the model based on the training data, ex. using Ordinary
  Least Squares to estimate \(\beta\)
\end{enumerate}

Reduces the problem of estimating a set of parameters. Choosing more
flexible models requires estimating more parameters and can lead to
overfitting.

\hypertarget{header-n16}{%
\subsubsection{Non-Parametric Methods}\label{header-n16}}

\begin{itemize}
\item
  Do not make explicit assumptions about the form of \(f\)
\item
  Able to fit a larger range of possible shapes for \(f\)
\item
  A very large number of observations are needed
\end{itemize}

\hypertarget{header-n25}{%
\subsubsection{Prediction Accuracy vs Model
Interpretability}\label{header-n25}}

Restrictive models are easier to understand. In linear models
(restrictive), it's easy to understand the relationship between the
parameters and the output, but it would be harder in a flexible model
like in generalized additive models.

When inference is the goal, there are advantages to using a restrictive
model. Even when focusing on prediction, more restrictive models can be
favored because flexible models have a higher chance of overfitting.

\hypertarget{header-n28}{%
\subsubsection{Supervised vs Unsupervised}\label{header-n28}}

Supervised learning has a response variable \(y_i\) to check the
accuracy of the model, but unsupervised does not. Unsupervised analysis
includes clustering, which performing analysis by grouping together
observations.

\hypertarget{header-n30}{%
\subsubsection{Regression vs Classification}\label{header-n30}}

Regression deals with estimating quantitative variables, such as the
value of a house. Classification deals with qualitative or categorical
variables, like gender.

\hypertarget{header-n32}{%
\subsection{Assessing Model Accuracy}\label{header-n32}}

\hypertarget{header-n33}{%
\subsubsection{Measuring the Quality of Fit}\label{header-n33}}

\[MSE = \frac{1}{n}\sum^n_{i=1}(y_i - \hat{f}(x_i))^2\]

The model needs to be flexible enough to model the structure of \(f\),
but not to overfit and model the noise of the data set. A low training
\(MSE\) and a high test \(MSE\) indicates overfitting.

\hypertarget{header-n36}{%
\subsubsection{Bias-Variance Trade-Off}\label{header-n36}}

\[E(y_0-\hat{f(x_0)})^2 = Var(\hat{f(x_0)}) + [Bias(\hat{f(x_0)})]^2 + var(\epsilon)\]

The expected test \(MSE\) can be broken down to the variance of
\(\hat{f}(x_0)\), the squared bias of \(\hat{f(x_0)}\) and the variance
of \(\epsilon\).

\emph{Variance} refers to the amount \(\hat{f}(x)\) would change if we
estimated it using a different training data set. More flexible models
have a higher variance.

\emph{Bias} refers to the error from approximating other data sets.

As the flexibility of the model increases, the bias decreases faster
than the variance increases, causing the \(MSE\) to decline. However, at
some point, increasing flexibility significantly increases the variance,
causing the \(MSE\) to increase.

\hypertarget{header-n42}{%
\subsubsection{The Bayes Classifier}\label{header-n42}}

\[Pr(Y=j|X=x_0)\]

Represents the conditional probability: the probability of \(Y = j\)
given the observed predictor \(x_0\). Assign to the class where \(j\) is
maximized.

If there are 2 classes, an instance is assigned to class \(1\) if
\(Pr(Y=1|X=x_0) > 0.5\).

It will produce the lowest possible test error rate (equivalent to the
irreducible error \(\epsilon\)) since it will maximize the conditional
probability. Bayes error rate is given by,

\[1 - E(max_jPr(Y=j|X))\]

Impossible for real data because we do not know the conditional
distribution.

\hypertarget{header-n49}{%
\subsubsection{K-Nearest Neighbors}\label{header-n49}}

\[Pr(Y=j|X=x_0) = \frac{1}{k}\sum_{i\in N_0}I(y_i=j)\]

Identifies \(K\) points in the training data that are closest to \(x_0\)
and estimates the conditional probability for class \(j\) as a fraction
of points in \(N_0\) whose response values equal \(j\). Then applies
Bayes rule and classifies the test observation based on the largest
probability.

Produces very accurate results that are close to Bayes Classifier.

A low value of \(K\) responds to low bias and high variance. High \(K\)
responds to low variance and high bias.

\end{document}
